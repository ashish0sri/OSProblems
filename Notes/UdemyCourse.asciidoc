Following Udemy OS course by Vignesh Sekar.
Adding Notes here + some of my own thoughts.

== Overview

Operating Systems act as resource managers, where resources can be anything like CPU time, memory,
etc. They contain various algorithms for resource management like scheduling of processes, etc.

OS gets loaded into RAM at boot time, and remains active till the system is active. OS space or
kernel space is the term used for the region in memory occupied by the OS. The remaining part is
called user space.

Program is the term used for the executables present on the hard disk. When launching the program,
a process is created like a copy in the hard disk; there could be multiple instances of the program
launched as separate processes. This could be loaded later into RAM for execution.
A process can be various states like new, ready, running, I/O, etc.

Every process has a PCB and some attributes associated with it. This is collectively also called as
context of the process.

Process Control Block or PCB is some memory space created for a process. It consists of Function
Call Stack (including stack variables), a heap for dynamic allocations, space for static and global
variables, and the actual program code. The Program Code is called Code Segment and rest is Data
segment.

Process Attributes like Process ID, Program Counter value, various register values, etc., help in
process save-restore.

== Scheduling Algorithms

Can be pre-emptive (utilizing save restore) or non pre-emptive (letting processes run to complete).
Scheduling Algorithms are only applied for processes in ready state.

If the expected time taken, arrival time info. is available then we can design some algorithm that
could minimize average waiting time, turnaround time, total schedule length, etc.
Gantt charts can be used to find these times above.
SJF (shortest job first) relies on expected time taken for a process to schedule.
SRTF (shortest remaining time first) also relies on the same, a bit like modification of SJF.
SRTF is premptive algorithm but SJF is non-premptive.
HRRN (Highest Response Ratio Next) is a non-premptive algorithm, which is a modification of SJF.
The response ratio is (Waiting Time + Running Time) / (Running Time). As a process waits for
longer durations, its response ratio will rise and could become higher than others with lower
running time. This can avoid the starvation problem where long running time processes can possibly
wait indefinitely for being scheduled.

Thought: In many real use cases the expected time taken information might not be available.

FCFS doesn't suffer from starvation (where there is a chance of a process waiting indefinitely).

Round Robin algorithm adds the concept of a "Time Quantum". Whenever a process takes the CPU for
time units equal to "Time Quantum", and there is some other ready process; preemption occurs.
All ready processes are kept in a FIFO queue, preempted process is added at the end of the queue.
The first ready process at the head of the queue is picked next for scheduling.

Higher time quantum can reduce context switches, but can cause larger waits for waiting processes.

FCFS and RR don't need the expected time taken information. RR avoids the "convoy effect" problem
present in FCFS where large time consuming process can block short time consuming process for a
long time.

Priority scheduling adds an additional priority to process attributes. These can be preemptive as
well.

A process state diagram can be created to see the state transition of a process, at various events.
Process can be sent to suspend state from ready state, when it is moved from RAM to hard disk; if
there is no more room in RAM for a new process but a high priority process comes in. Later it can
come back to ready space when RAM is available.

After scheduler decides what process to schedule next, it calls the dispatcher which loads the
various registers with process attributes to execute further.

== Memory Allocation Techniques

Actual RAM addresses are physical address, these addresses lie in what we call as physical address
space.
A process also has addresses for various things like functions, etc., these are in a logical
(or virtual) address space.

=== Contiguous Allocation

Contiguous allocation is if the addresses of the process are placed in a contiguous region of RAM
physical addresses. The RAM can be made to use fixed/variable partitioning. In fixed partitioning,
before a process is loaded, the RAM is split into some partitions. Processes are placed into one
suitable partition when being loaded to RAM. A partition can't hold more than 1 process. In variable
paritioning, the RAM is not split into any partitions beforehand. When a process is loaded, it gets
a new partition which has the required amount of memory that it needs.
There is an internal fragmentation problem in fixed partitioning, where there could be an extra size
in the partition for a process that it doesn't need, but it got because the partition was created
beforehand. There is an external fragmentation problem in variable partitioning, where based on
how processes came and went earlier, there might be enough space available to accommodate a new
process but it is not contiguous and thus the new process can't be allocated in RAM.

Compaction and Paging are used to avoid fragmentation problems. Compaction causes the allocated
regions to be made contiguous, at regular intervals; so that the free space also becomes
contiguous. But compaction is a time consuming operation, and the CPU is running compaction code
instead of running actual user processes.

To convert logical address to physical address in contiguous allocation, CPU can simply keep a
value in the base register which corresponds to the base address for the process, and just add
this value to the logical address to get the physical address. This means the translation is
just an addition. Additionally a table will need to be kept in RAM for process id to process base
address information.

=== Non Contiguous Allocation

Paging is used for non-contiguous allocations and can solve the fragmentation problems observed
with earlier methods described above. However, translation from logical to physical address is a
relatively complex.

RAM is divided into fixed size frames. Process memory is also divided into similar sized chunks
called as pages. The frame size is kept same as page size, this becomes the allocation granularity,
of which small contiguous chunks of memory can be allocated. Pages can be placed into frames that
can be in any order. A Page Table is kept for every process, which has many as entries as the number
of pages in the process. The entry for a particular page contains the frame number where it has been
allocated in RAM. CPU needs to look up the page table of the process to find the physical address
for a logical address.

The logical address can be divided into two parts - page number (MSBs indicating the page) and page
offset (LSBs indicating the offset within that page). To find the physical address, simply the page
table for the process needs to be traversed, and the frame number for that page number can be
found. Replacing the page number with the frame number, gives the physical address for the logical
address.

Internal fragmentation is possible in paging, but the effect is much smaller as page size is small
compared to the size of a partition in contiguous allocation with fixed partitioning.

Page table entry size is number of bits required to repesent a frame, i.e. the number of bits for
the frame number. Number of page table entries is total number of pages in the process. Total page
table size is the product of the two.

==== Multilevel Paging

Page tables can become very large with single level paging, e.g. with 4KB page size and 32-bit
logical addresses, there are 2^(32-12)^ = 2^20^ pages possible in the process, This is a huge number
of page table entries for a process. We typically want to fit a page table in a RAM frame itself.
This enables directly indexing into the page table, which is not possible if page table uses more
than 1 frame.

Multiple level of page tables help solve the problem above by creating more levels that can point
to lower level page tables, so that each of the page tables can fit within a RAM frame. Higher
level page table is looked into first, there indexing using some bits in page number provide the
frame number where next level of page table is present. After reaching the frame for the next
level, we can use the next few bits in page number to find the frame number where the next level
after that is present. At the lowest level, we simply get the frame number of the RAM page which
is used to find the physical address for the page.

An example below:

* With page size = 4KB, number of bits needed for page offset is 12.
* Assume that the RAM size is 4GB.
** Number of frames in RAM will be (4GB/4KB = ) 1M.
** 20 bits are needed for frame number that will be stored in the page table entry (PTE).
** If we can keep only multiples of bytes then, each PTE will take 3 bytes.
* With 4KB page size, each Page Table that fits in RAM can have 4KB/3 = nearly 1365 entries.
** Restricting to a power of 2, we will further limit this to 1024 or 2^10^ entries.
** So 10 bits will be needed for the lowest (first) level of Page Table.
* For a process with 32-bit logical addresses, 20 bits will be used for the page number.
** First (lowest) level will use the 10 LSBs of the page number.
** Second (next) level will use the next 10 bits.
*** This covers the entire 20 bits, so 2 levels are sufficient.
** The address translation will first check using 10 MSBs of page number in the second level table.
*** There will be only one second level table with 2^10^ entries.
*** Using this we find the frame number where the first level page table for the address is stored.
** In the first level table, we search using the next 10 bits (LSBs here) of the page number.
*** This gives the frame number where actual data is stored.
*** We can replace the page number by this frame number, and get the physical address in RAM.
*** There are 2^10^ page tables in the first level.
** All page tables together take (1 + 2^10^) * 4KB = nearly 4MB space for the process.
*** This is larger than 1MB * 3 space that a single page table would have taken.
*** But each can fit in 1 page, so indexing is possible in each of them.
** There are 3 memory accesses required for each actual accesses here.
** Space taken by page tables can be optimized by keeping some second level table entries as NULL.
*** This will be for the address regions that aren't allocated yet.
* For a process with 64-bit logical addresses, 52 bits will be used for the page number.
** First level has 10 bits, same for 2nd, 3rd, 4th and 5th levels, and 2 MSBs for 6th.
** 7 memory accesses are required for each actual access.
** 4KB * (1 + 2^2^ * (1 + 2^10^ * (1 + 2^10^ * (1 + 2^10^ * (1 + 2^10^))))) is the space taken by
the page tables = nearly 4.4 * 4096 TB !
*** Single page table would have required 2^52 entries, which needs 3 * 4096 TB !
** Here too space taken by page tables can be optimized by keeping some level table entries as NULL.

It is also possible for the OS or kernel driver to try to minimize the memory accesses required to
access a page by choosing fewer page table levels, and artifically limiting the logical address
space of a process.

Page Table Entries can have additional bits other than the frame number.
These could be:

. Access Protection Bits (Read Write, Read Only, No Access).
. Present/Absent Bit, whether the corresponding page is currently present on RAM or not.
. Referenced Bits, used in page replacement, i.e. whether page was referred recently or not.
. Dirty Bits, used when swapping page out to hard disk, for whether the page needs to be written.

== Virtual Memory

A Principle of Locality, says that at anytime a process will only require few pages and these set of
pages will change overtime. So we need not keeps all pages of a process in the RAM everytime. These
could just be kept present in hard disk only.

Such a memory system where only few pages need to be maintained in RAM, is called as virtual memory.
The set of addresses of the process which are inside the RAM are called as (Resident) virtual
address space.

This increases degree of multiprogramming, and allows running of processes that need memory even
more than the physically available memory in RAM.

=== Page Fault

Page fault occurs when the logical address of the process doesn't have a page in the RAM. When the
page fault is handled a relevant page for that address in hard disk is brought into the RAM, and
the access that caused the page fault is replayed. The present/absent bit in page table entry can
give the info whether the access is hit in RAM or a fault happens.

Average Memory Access Time is the average time taken access a byte in the memory. In a single level
page table system, 2 RAM access are always needed; one for access in page table and another for
actual access (which could be after a fault). If x is the ratio of page faults then additionally x *
Hard Disk Access Time is also needed. So the AMAT can be calulated as 2 * RAM Access Time + x * Hard
Disk Access Time. An example, if RAM access time is 5ns, Hard Disk Access Time is 500ns and x is 0.1
then AMAT will be 2 * 5 + 0.1 * 500 = 60ns.

== TLB

TLB stands for Translation Look-Aside Buffer. There is additional overhead of accessing page tables
when accessing memory, as one extra memory access is required per page table level. We can maintain
a few pages (or lines) of our page table in some cache to avoid this overhead and reduce the AMAT.
The buffer where this is maintained is called as TLB. Here we can directly keep frame number for a
logical address of the process. The TLB access time is even less than RAM access time as it is in
some cache.

== Frame Allocation

Frames are allocated in RAM for some number of pages of a process. The pages of the process go to
RAM go into those frames. When a new page needs to go to a RAM frame and all frames for that process
are full, some page replacement algorithm is used to select a page in RAM that needs to be replaced.
If we allocate too few frames for a process, then it is likely that there are too many page faults,
which leads to Thrashing, i.e. too much time being spent in page faults.

There are two types of frame allocation methods. One is static where we either divide the frames in
RAM equally amongst the processes, or divide the frames based on process size, or divide based on
something else. Dynamic Methods, are other ones where based on requirements the number of frames for
processes can be changed while they are still running.
Note: A process with more pages overall doesn't necessarily need more frames in RAM. e.g. a process
that needs to access more different pages in a short time will need more frames in RAM, but a
process that mostly accesses the same pages in a short time doesn't need more frames in RAM.

== Page Replacement Algorithms

There are two types of page replacement, local and global. Local ones only replace the pages in RAM
that belong to the same process. Global ones can replace the pages in RAM which might belong to some
other process as well. Local replacement is generally preferred as it doesn't interfere with other
processes.

In Demand Paging, no page is loaded into RAM until CPU requests for a byte in that page. This causes
a page fault for every page.

Various page replacement algorithms are LRU (least recently used), FIFO (first in first out), MRU
(most recently used) etc. If we have information of pages to be accessed in future (which is not the
case practically in general), another algorithm called as Optimal can be used.

== Synchronization and Deadlocks

=== Concept of Shared Memory

For process address space protection from other processes, OS disallows (or doesn't provide a way
to) a process from accessing another's PCB. But there are cases where two or more processes might
need to share some memory. This region is called as "Shared Memory" which could be considered as
being kept in a separate region from PCBs.

=== Need for Synchronization

High-level instructions get split up in assembly code, e.g. a simple increment gets split into
LOAD_TO_REG, INCR_REG_VALUE, STORE_FROM_REG. Due to preemption all of these might not be done but
some of them might be, before context switch to another process. If the other process accesses a
memory shared with the one switched out, it can load a value in its register which might have been
different if all the machine code instructions meant for the high level instruction got complete.
This process can then perform its operation based on the value it loaded, and store the result in
the same shared memory. Whenever the original process gets back to execution, it has already loaded
a value before the store from the other process, and it performs its operation based on the older
value. When it stores the result it calculated, it simply overwrites the effect of the operation
done by the other process.

=== Critical Section

Critical section is the region of code in a process where the accesses impact the contents of the
region of memory shared with other processes. The code performing "increment" in the example above
is a an example of a Critical Section.

"Race Condition" is said to happen when the order of execution of instructions of two processes
determines the outcome of value of some shared memory. In the example above, if similar "increment"
code for two processes have their assembly level instructions overlapped with each other, we can
see that an incorrect value (effect of only 1 increment). However, if there is no overlap, i.e. all
assembly level instructions of one process complete before the other process instructions begin, we
will see the correct value. Here the order of execution is determining whether we see correct or
incorrect value at the end of the "increment" code for both processes, hence there is a race
condition.

It can be seen that Critical Sections in two different processes that impact same shared memory,
being executed by the two process simultaneously; can cause race conditions. We can avoid race
condtions simply by not letting any two processes be in the Critical Sections that impact same
shared memory.

=== Synchronization Mechanisms

To avoid having processes enter CS at same time, we add an "Entry Section" before and an "Exit
Section" after the CS in all processes. Entry section checks if it is safe to enter CS, i.e. no
other process is already in it. Exit section releases the sole control of CS so that another process
can acquire it.

There are various synchronization mechanisms and we see if they satisfy some conditions. Three major
ones are "Mutual Exclusion", "Progress", "Bounded Waiting". Mutual Exclusion is said to exist, if
only one process can be in CS for some shared memory at one time. Progress exists if a process in a
non-CS cannot block a process from entering a CS. Bounded Waiting exists, if once a process requests
for entering a CS, there is an upper bound on number of times other processes are allowed to enter
the CS for the same shared memory before its request is served.

Mutual Exclusion is enough to guarantee no race conditions. Progress makes sure that a process is
blocked from entering a CS only if some other process is in it. Bounded Waiting makes sure that a
process waits a finite amount of time before it gets to access the CS.

The more of these conditions are met by a synchronization mechanism, the better it is considered in
general.

=== Busy Waiting Synchronization Mechanisms

==== Lock Variable

* Lock Variable is a simple CS.
* Its entry section is like { while (lock != 0);lock=1; } and exit section is like { lock = 0; }.
* It doesn't satisfy mutual exclusion, as a preempt between while and lock=1 can still cause more
that one process to enter the CS at the same time.
** Thus, it can still cause race conditions, and is not considered as a good synchronization
mechanism.
* It doesn't satisfy Bounded Waiting condition.
* It satisfies Progress condition.
* Other features are it is Busy Waiting, Usermode implementable, needs no special H/W support.

==== TSL

* TSL stands for "Test and Set Lock" instruction.
** This needs special H/W support for the new instruction.
* Entry Section is { while(TSL(lock)); } and Exit Section is { lock = 0; }.
** This sets the value in lock to 1 unconditionally but returns the older value of lock before it
stored.
** If the older value of lock read by a process is 0, it can carry on into the CS, otherwise it
keeps waiting till it sees 0.
* It satisfies Mutual Exclusion as TSL is one instructions in assembly level as well so can't be
split up.
* It doesn't satisfy Bounded Waiting condition.
* It satisfies Progress condition.
* Other features are it is Busy Waiting, Usermode implementable, Needs special H/W support.

==== Strict Alternation

* Strict Alternation Mechanism is a turn based mechanism that works for two processes only.
* Here a process takes a lock only if it is its turn to take the lock and after releasing the lock
it assigns the turn to another process.
* One process has Entry Section as { while(turn == 0); } and Exit Section has { turn = 0; }.
** This makes it wait for turn to become 1 and then enter CS and when exiting makes turn 0 to let
another process enter.
** The other process is waiting for turn to become 0 to enter CS and when exiting will make turn 1
to let the original process enter again.
* It doesn't satisfy progress condition as a process in non-CS can block another process from
entering CS.
** If the process in non-CS has its turn, it keeps the other process from taking the CS. The other
process can continue only after the original process gets into CS and exits it.
* Other features are it is Busy Waiting, Works for only 2 processes, Usermode implementable, Needs
no special H/W support.

==== Disable Interrupts

* This simply disables scheduler interrupts that makes context switching not happen and doesn't let
another process run till it renables interrupts.
* Before entering CS interrupts are disabled, after exiting the interrupts are re-enabled.
* Gives too much control to user, affects entire system as it stops other process from executing
completely.
** So this is considered a dangerous mechanism and generally not used anywhere.

==== Peterson Method

* This is a mechanism that works for two processes.
* This uses a mix of turn-based and interested variable techniques.

.Entry Section and Exit Section functions
----
void EntrySection(int process)
{
  int other = 1 - process;
  Interested[process] = true;
  LastTurnUpdater = process;

  while (Interested(other) == true && LastTurnUpdater == process);
}

void ExitSection(int process)
{
  Interested[process] == false;
}
----
* Here Interested[2] and LastTurnUpdater are global variables that both processes can access and
are backed by same memory for both processes.
* We can see that if the other process was found to be not interested then the condition in while
is always false and the process can enter the CS.
* Only if the other process is interested we need to resolve a conflict. The conflict is resolved
by the LastTurnUpdater variable. If the current process is last to have set it, then it doesn't
exit the while loop. But the other process can then exit the while loop and enter CS. After exiting
the CS, it makes itself as not interested, which lets the original process exit the while loop.
* Mutual Exclusion is guaranteed, as if 1 process enters the CS, the interested for it is definitely
true. So the other will always see the first of while conditions as true. For the second condition:
** If the original process saw Interested(other) as true then if LastTurnUpdater is original
process, then it could not have entered the CS first due to the condition. But since we know that it
has entered, LastTurnUpdater can only be the other process.
*** Note: The process in CS cannot change the LastTurnUpdater, so only the other one can change it.
** If original process saw Interested(other) as false and entered the CS with LastTurnUpdater as the
original process. It is clear due to order of execution, LastTurnUpdater was original process first,
then the check for Interested(other) showing as false means that the other process did not set it to
true before original process set LastTurnUpdater. That means that the other process will set the
LastTurnUpdater to the other process after the original process has already set LastTurnUpdater
earlier. The original process is in CS so it can't modify it, so LastTurnUpdater at the check can
still only be the other process.
** This means if the original process is in CS, both conditions in while in Entry Section for other
process is true, and the other process will not be able to exit the loop and enter the CS. This
guarantees Mutual Exclusion.
* Progress is also guaranteed as a process in non-CS can't block another from entering CS.
* It satisfies Bounded Waiting as well, since a waiting process can be LastTurnUpdater only for as
long as the process in CS exits the CS and sets LastTurnUpdater to itself. As soon as the waiting
process is not the LastTurnUpdater, it will get a chance to enter the CS before the other one.
* Other features are it is Busy Waiting, Works for only 2 processes, Usermode implementable, Needs
no special H/W support.

=== Disadvantages of Busy Waiting

One obvious disadvantage is that the process in busy waiting loop keeps consuming CPU cycles
continuously till it gets out of it. If that can be sent to sleep state and woken up again once the
CS is free from use by all other processes, this time could be saved.

Another is "Priority Inversion", where a process with higher priority might get scheduled and that
causes preempt of a running process that is in a CS that the new higher priority process requests
for. The higher priority process can't make progress until the lower priority one exits the CS; and
the lower priority one just got preempted out.

=== Non-Busy Waiting Synchronization Mechanisms

These are also called as Sleep-Wakeup or Block-Wakeup mechanisms. In the entry section a sleep();
system call is added, if the process can't enter the CS based on the condition. When the process
in CS exits the CS, wakeup() system call will be called for the process that called sleep().

The process sent to Sleep state using sleep(); aren't picked by the scheduler, so they don't waste
CPU time. When wakeup signal is sent, the process gets back to Ready state and can be scheduled
again.

==== Binary Semaphores

A simple Non Busy Waiting method uses sleep() when the process is in Entry Section and wakeup() is
called for it in Exit Section for another process. Assume that the process that was to wait decided
to go to sleep based on some condition but has not yet executed sleep(). Assume it now gets a
wakeup() signal; nothing will happen as it is in Ready state already. Later when it goes to the
Sleep state, it might never get the wakeup() signal as it had received that earlier and ignored it.
This can lead to another form of Deadlock where progress is blocked. So such a simple mechanism does
not work.

A solution for this problem is to use a mechanism called as binary semaphore (also known as mutex).
This basically combines the if condition check and sleep system call to a single "wait()" system
call, which takes in a "binary semaphore" type struct as input. The update of variable waited on
and wakeup system call is combined into a "signal()" system call. The signal() and wait() system
calls are never preempted by the OS.

The structure type, and functions pseudo-code are something like:

.Binary Semaphore Struct
----
struct binSemaphore {
  bool value;
  QueueType queue;
};
----

.wait() system call
----
void wait(struct binSemaphore s)
{
  if (s.value == 1) {
    s.value = 0;
  }
  else {
    addToQueue(s.queue, currentProcess);
    sleep();
  }
}
----

.signal() system call
----
void signal(struct binSemaphore s)
{
  if (queueIsEmpty(s.queue) {
    s.value = 1;
  }
  else {
    processToWakeUp = removeFromQueue(s.queue);
    wakeup(processToWakeUp);
  }
}
----

* s.value here indicates whether the process can go on to execute CS directly (s.value = 1) or it
needs to go to sleep state first (s.value = 0). 
* We can assume that no two processes are executing sleep() and wakeup() at the same time. That is
the OS allows only one process to execute these at one time.
* Since these system calls are not preempted, the process calling wait() will definitely go to sleep
if the signal value it sees is 0.
* When the process in CS goes on to call signal(),
** If the queue is not empty, it will wake up the first process that called wait().
*** It still keeps the s.value as 0 so that the next process to wait first gets to the end of the
queue, rather than getting into CS directly.
** If the queue is empty, it sets s.value to 1 so that the next process to wait can directly get
into the CS.
* It can be easily seen that this satisfies Mutual Exclusion, Progress and Bounded Waiting.
* Other features are: Needs Kernel Mode Implementation, Needs No Special H/W support, works for > 2
processes as well.
** It is obviously not Busy Waiting.
* Binary Semaphore is also called as Mutex as they are used generally whereever Mutual Exclusion is
required.

===== Dining Philosopher Problem

This is a problem that can be used to explain some mutex acquisition order that can lead to a
deadlock. Assume that there are four philosophers sitting at a circular dining table, and each of
the them follows the same order of execution of their dining process. Assume that there is 1 fork
between each of the philosophers and 2 forks are needed to eat the food. The order of execution of
all philosophers is:

----
1. Take the fork on the left.
2. Take the fork on the right.
3. Eat using both the forks.
4. Drop the left fork.
5. Drop the right fork.
----

Assume that all philosophers do the first step at the same time. Now, none of them can perform the
second step as all forks have been taken up one each by all the philosophers, so there is no fork
left on the right that they can pick up. So here is a deadlock, where none of the philosophers can
continue.

One easy solution of the problem is to have one philosopher reverse steps 1 and 2. If we say that
the fourth philosopher reverses steps 1 and 2, only 1 of P1 or P4 can pick the fork that is between
them. If P4 can pick up the right fork, then one of P3 or P4 will be able to pick the other fork in
step 2. If P1 picked up that fork, then P3 can pick up the other fork in step 2 as P4 didn't pick it
up in step 1. Once one of the philosopher can go to step 3, they will also go to step 5 and drop
both the forks eventually. And it is easy to see beyond that all philosophers will be unblocked
sooner or later.

If we replace philosophers by processes P1, P2, P3, and P4 and fork to left by mutexes M1, M2, M3,
and M4; and fork to right then become mutexes M2, M3, M4, and M1. We can see that if a process P(i)
tries to acquire 2 mutexes (gets in CS of) in order M(i), then M( (i+1)%4 ); a similar deadlock
problem can occur which can be solved by making one of the process reverse the order.

Another way to avoid deadlock could be if we do not let a process that has acquired mutex M(i)
acquire a mutex M(j) where j < i. If the process was acquiring in that order, it can simply flip
the order. Here P4 was acquiring in order M4, M1; we can reverse this and make it acquire in order
of M1, M4 so that i < j condition is always met.

==== Counting Semaphores

Unlike binary semaphores, counting semaphores can take any integer value instead of just the boolean
0 or 1. For synchronization using wait() and signal(), these can allow some number of processes to
enter the critical section at once based on an initial value. This is not meant to guarantee mutual
exclusion but to limit the number of processes that can be in a CS.

The wait() and signal() code is a bit like:

.wait() system call
----
void wait(struct countSemaphore s)
{
  s.value--;
  if (s.value < 0) {
    addToQueue(s.queue, currentProcess);
    sleep();
  }
}
----

.signal() system call
----
void signal(struct countSemaphore s)
{
  s.value++;
  if (s.value <= 0) {
    processToWakeUp = removeFromQueue(s.queue);
    wakeup(processToWakeUp);
  }
}
----

* Let's say n was the initial value of s.value.
** s.value cannot exceed n if total wait() called is > total signal() called, which should be the
case when using this for a CS.
* At any point in time positive value of s.value means (n - s.value) processes have entered the CS
and s.value processes can further enter the CS.
** Applies for s.value = 0 as well.
* A negative value of s.value means n processes are in CS, and (0 - s.value) processes are in the
queue to enter the CS.

Note: Counting semaphores using signal and wait can be used for purposes other than limiting
processes in a critical section as well. One example is Producer Consumer problem with N (> 1)
slots.

=== Deadlocks

Resources are any physical or virtual (like semaphores, CS) commponents in a computer which is
available in limited quantity. Processes keep attempting to acquire these resources. If a situation
comes up where two or more processes are unable to proceed because each of them is waiting for one
of the others to do something, none of these processes can make any progress. This situation is
called as Deadlock.

There is another problem called as Starvation where a runnable process is overlooked indefinitely by
the scheduler.

The necessary conditions for a Deadlock are:

* Mutual Exclusion: Only one process accesses a resource at one time.
* Hold and Wait: Process can hold some resource and wait for another.
* No Preemption of Resource: The Resource Access can't be preempted and assigned to some other
process.
* Circular Wait: If we create a graph where there is a directed edge from a process waiting for
resource pointing to the process that holds the resource, and we create all such edges; if there is
a cycle in this graph, then circular wait condition is true.

Minimum number of resources needed given N processes, such that each process needs K instances of
the resource can be found out by starting with giving K-1 resources to each process, and then adding
one more instance so that at least one process gets required number of resources. So this will be
(K-1)*N + 1.

==== Deadlock Handling Mechanisms

One method is Deadlock prevention, i.e. not let deadlock even happen. There are 4 conditions that
all need to be met for deadlock to happen. If we not allow any one of these condition to happen the
deadlock won't happen. It can be seen that disallowing circular wait is the only reasonable method
for this. By giving some ordering number to resources, and forcing all process to acquire resources
only in the increasing ordering number of resources, the circular wait can be prevented. If process
needs to acquire a resource that has a lower ordering number than that of a resource it already has,
then it will release all its resources, and acquire them again in the order of increasing ordering
numbers.

Another is Deadlock avoidance, if we know the future resource demand by various processes, we can
figure out whether a deadlock situation can arise and change resource allocation order such that the
deadlock won't happen. There are some algorithms like Banker's algorithm that can be used for this
(Note: not going into details in these notes).

Another is Deadlock detection and recovery, if we can run some code at some intervals of time to
check if a deadlock has happened already, we can take some action like killing one of the processes
in deadlock and then check later again if a deadlock still happens. This can be done till the
deadlock doesn't happen anymore.

All of the above methods need some extra code for deadlocks. In general for most common OS for
general purpose computers, simply let the deadlock happen and ignore it. The user can restart the
whole system to get back to a sane state. Deadlocks are generally rare and for general use cases
restarting isn't a major problem, so the common OS chooses to ignore it.


